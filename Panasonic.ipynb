{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pratik/exp/1/CenterPoint/det3d/datasets/waymo/waymo_common.py:17: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mayavi.mlab as mlab\n",
    "%gui qt\n",
    "import argparse\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "try:\n",
    "    import apex\n",
    "except:\n",
    "    print(\"No APEX!\")\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from det3d import torchie\n",
    "from det3d.datasets import build_dataloader, build_dataset\n",
    "from det3d.models import build_detector\n",
    "from det3d.torchie import Config\n",
    "from det3d.torchie.apis import (\n",
    "    batch_processor,\n",
    "    build_optimizer,\n",
    "    get_root_logger,\n",
    "    init_dist,\n",
    "    set_random_seed,\n",
    "    train_detector,\n",
    ")\n",
    "from det3d.torchie.trainer import load_checkpoint\n",
    "import pickle \n",
    "import time \n",
    "from matplotlib import pyplot as plt \n",
    "from det3d.torchie.parallel import collate, collate_kitti\n",
    "from torch.utils.data import DataLoader\n",
    "from pyquaternion import Quaternion\n",
    "import matplotlib.cm as cm\n",
    "import subprocess\n",
    "import cv2\n",
    "from tools.demo_utils import visual \n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import itertools\n",
    "import pickle as pkl\n",
    "def load_pkl_dictionary(pkl_fpath):\n",
    "    with open(pkl_fpath, \"rb\") as f:\n",
    "        return pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from argoverse.utils.json_utils import read_json_file, save_json_dict\n",
    "from argoverse.utils.pkl_utils import load_pkl_dictionary, save_pkl_dictionary\n",
    "import torch.distributed as dist\n",
    "\n",
    "from centerpoint.utils.config import Config\n",
    "from centerpoint.registry import DETECTORS\n",
    "\n",
    "from centerpoint.dataset.centerpoint_dataloader import build_dataloader\n",
    "from centerpoint.nuscenes_dataset import NuScenesDataset\n",
    "from centerpoint.nuscenes_dataset import Reformat\n",
    "from centerpoint.pcd import PCDDataset\n",
    "from centerpoint.utils.checkpoint import load_checkpoint\n",
    "from centerpoint.utils.compose import Compose\n",
    "from centerpoint.utils.loading import LoadPointCloudAnnotations, LoadPointCloudFromFile\n",
    "from centerpoint.utils.preprocess import AssignLabel, Preprocess\n",
    "from centerpoint.utils.test_aug import DoubleFlip\n",
    "from centerpoint.utils.voxel_generator import Voxelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist_info():\n",
    "    if torch.__version__ < \"1.0\":\n",
    "        initialized = dist._initialized\n",
    "    else:\n",
    "        initialized = dist.is_initialized()\n",
    "    if initialized:\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "    else:\n",
    "        rank = 0\n",
    "        world_size = 1\n",
    "    return rank, world_size\n",
    "\n",
    "\n",
    "def get_root_logger(log_level=logging.INFO):\n",
    "    logger = logging.getLogger()\n",
    "    if not logger.hasHandlers():\n",
    "        logging.basicConfig(\n",
    "            format=\"%(asctime)s - %(levelname)s - %(message)s\", level=log_level\n",
    "        )\n",
    "    rank, _ = get_dist_info()\n",
    "    if rank != 0:\n",
    "        logger.setLevel(\"ERROR\")\n",
    "    return logger\n",
    "\n",
    "\n",
    "def example_to_device(example, device=None, non_blocking=False) -> dict:\n",
    "    \"\"\" \"\"\"\n",
    "    assert device is not None\n",
    "\n",
    "    example_torch = {}\n",
    "    float_names = [\"voxels\", \"bev_map\"]\n",
    "    for k, v in example.items():\n",
    "        if k in [\"anchors\", \"anchors_mask\", \"reg_targets\", \"reg_weights\", \"labels\"]:\n",
    "            example_torch[k] = [res.to(device, non_blocking=non_blocking) for res in v]\n",
    "        elif k in [\n",
    "            \"voxels\",\n",
    "            \"bev_map\",\n",
    "            \"coordinates\",\n",
    "            \"num_points\",\n",
    "            \"points\",\n",
    "            \"num_voxels\",\n",
    "        ]:\n",
    "            example_torch[k] = v.to(device, non_blocking=non_blocking)\n",
    "        elif k == \"calib\":\n",
    "            calib = {}\n",
    "            for k1, v1 in v.items():\n",
    "                calib[k1] = torch.tensor(v1).to(device, non_blocking=non_blocking)\n",
    "            example_torch[k] = calib\n",
    "        else:\n",
    "            example_torch[k] = v\n",
    "\n",
    "    return example_torch\n",
    "\n",
    "\n",
    "def parse_second_losses(losses):\n",
    "    \"\"\" \"\"\"\n",
    "    log_vars = OrderedDict()\n",
    "    loss = sum(losses[\"loss\"])\n",
    "    for loss_name, loss_value in losses.items():\n",
    "        if loss_name == \"loc_loss_elem\":\n",
    "            log_vars[loss_name] = [[i.item() for i in j] for j in loss_value]\n",
    "        else:\n",
    "            log_vars[loss_name] = [i.item() for i in loss_value]\n",
    "\n",
    "    return loss, log_vars\n",
    "\n",
    "\n",
    "def batch_processor(model, data, train_mode, **kwargs):\n",
    "    \"\"\" \"\"\"\n",
    "    if \"local_rank\" in kwargs:\n",
    "        device = torch.device(kwargs[\"local_rank\"])\n",
    "    else:\n",
    "        device = None\n",
    "\n",
    "    example = example_to_device(data, device, non_blocking=False)\n",
    "\n",
    "    del data\n",
    "\n",
    "    if train_mode:\n",
    "        losses = model(example, return_loss=True)\n",
    "        loss, log_vars = parse_second_losses(losses)\n",
    "\n",
    "        outputs = dict(\n",
    "            loss=loss, log_vars=log_vars, num_samples=len(example[\"anchors\"][0])\n",
    "        )\n",
    "        return outputs\n",
    "    else:\n",
    "        return model(example, return_loss=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_detector(logger, cfg, train_cfg=None, test_cfg=None):\n",
    "\n",
    "    registry = DETECTORS\n",
    "    default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg)\n",
    "    return build_from_cfg(logger, cfg, registry, default_args)\n",
    "\n",
    "\n",
    "def is_str(x):\n",
    "    \"\"\"Whether the input is an string instance.\"\"\"\n",
    "    return isinstance(x, six.string_types)\n",
    "\n",
    "def build_from_cfg(logger, cfg, registry, default_args=None):\n",
    "    \"\"\"Build a module from config dict.\n",
    "    Args:\n",
    "        cfg (dict): Config dict. It should at least contain the key \"type\".\n",
    "        registry (:obj:`Registry`): The registry to search the type from.\n",
    "        default_args (dict, optional): Default initialization arguments.\n",
    "    Returns:\n",
    "        obj: The constructed object.\n",
    "    \"\"\"\n",
    "\n",
    "    from center_head import CenterHead\n",
    "    from centerpoint.models.scn_backbone import SpMiddleResNetFHD\n",
    "    from centerpoint.models.rpn import RPN\n",
    "    from centerpoint.models.voxel_encoder import VoxelFeatureExtractorV3\n",
    "    from centerpoint.models.voxelnet import VoxelNet\n",
    "\n",
    "    reader = VoxelFeatureExtractorV3(\n",
    "        num_input_features = 5,\n",
    "        norm_cfg = None\n",
    "    )\n",
    "    backbone = SpMiddleResNetFHD(\n",
    "        num_input_features = 5,\n",
    "        ds_factor = 8,\n",
    "        norm_cfg = None\n",
    "    )\n",
    "    neck = RPN(\n",
    "        layer_nums = [5, 5],\n",
    "        ds_layer_strides = [1, 2],\n",
    "        ds_num_filters = [128, 256],\n",
    "        us_layer_strides = [1, 2],\n",
    "        us_num_filters = [256, 256],\n",
    "        num_input_features = 256,\n",
    "        norm_cfg = None,\n",
    "        logger = logger # <Logger RPN (INFO)>\n",
    "    )\n",
    "    bbox_head = CenterHead(\n",
    "        in_channels = sum([256, 256]),\n",
    "        tasks=[\n",
    "            dict(num_class=3, class_names=['VEHICLE', 'PEDESTRIAN', 'CYCLIST']),\n",
    "        ],\n",
    "        dataset = 'waymo',\n",
    "        weight = 0.25,\n",
    "        code_weights = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2, 1.0, 1.0],\n",
    "        common_heads = {\n",
    "            'reg': (2, 2),\n",
    "            'height': (1, 2),\n",
    "            'dim': (3, 2),\n",
    "            'rot': (2, 2),\n",
    "            'vel': (2, 2)\n",
    "        },\n",
    "        share_conv_channel = 64,\n",
    "        dcn_head = False,\n",
    "    )\n",
    "\n",
    "    detector = VoxelNet(\n",
    "        reader=reader,\n",
    "        backbone=backbone,\n",
    "        neck=neck,\n",
    "        bbox_head=bbox_head,\n",
    "        train_cfg=None,\n",
    "        test_cfg=SimpleNamespace(**default_args['test_cfg']),\n",
    "        pretrained=None\n",
    "    )\n",
    "    return detector\n",
    "\n",
    "\n",
    "def build_dataset(cfg, args):\n",
    "    \"\"\" \"\"\"\n",
    "    nsweeps = cfg.nsweeps\n",
    "    dataset_name = cfg.type\n",
    "    split = args['split']\n",
    "    if split == 'test':\n",
    "        info_path = f'data/{dataset_name}/infos_test_{str(nsweeps).zfill(2)}sweeps_withvelo.pkl'\n",
    "    else:\n",
    "        info_path = f'data/{dataset_name}/infos_val_{str(nsweeps).zfill(2)}sweeps_withvelo_filter_True.pkl'\n",
    "    info_path = cfg.root_path\n",
    "\n",
    "    pipeline = [\n",
    "            LoadPointCloudFromFile(dataset = 'PCDDataset')\n",
    "    ]\n",
    "    if split != 'test':\n",
    "        # only relevant for train or val\n",
    "        pipeline += [LoadPointCloudAnnotations(with_bbox = True)]\n",
    "\n",
    "    pipeline.extend([\n",
    "            Preprocess(\n",
    "                cfg=SimpleNamespace(**{\n",
    "                    'mode': 'val',\n",
    "                    'shuffle_points': False,\n",
    "                    'remove_environment': False,\n",
    "                    'remove_unknown_examples': False\n",
    "                })\n",
    "            ),\n",
    "            Voxelization(\n",
    "                cfg = cfg['pipeline'][3]['cfg']\n",
    "            ),\n",
    "            AssignLabel(\n",
    "                cfg = cfg['pipeline'][4]['cfg']\n",
    "            ),\n",
    "            Reformat(double_flip=False) #Changed from True\n",
    "        ])\n",
    "    dataset = PCDDataset(\n",
    "        info_path = info_path,\n",
    "        root_path = \"data/Panasonic/processed/lidar\",\n",
    "        test_mode = True,\n",
    "        class_names = cfg.class_names,\n",
    "        nsweeps = nsweeps,\n",
    "        ann_file = info_path,\n",
    "        pipeline = pipeline\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def load_opts():\n",
    "    \"\"\" \"\"\"\n",
    "    opts_dict = {\n",
    "        'config': 'configs/mixed_pcd.py',#nusc_centerpoint_voxelnet_dcn_0075voxel_flip_testset.py',\n",
    "        'work_dir': 'work_dirs/mixed_wn',\n",
    "        'checkpoint': 'work_dirs/mixed_wn/latest.pth',\n",
    "        'txt_result': False,\n",
    "        'gpus': 1,\n",
    "        'launcher': 'none',\n",
    "        'speed_test': True,\n",
    "        'local_rank': 0,\n",
    "        'testset': False\n",
    "    }\n",
    "    opts = SimpleNamespace(**opts_dict)\n",
    "\n",
    "    if \"LOCAL_RANK\" not in os.environ:\n",
    "        os.environ[\"LOCAL_RANK\"] = str(opts.local_rank)\n",
    "\n",
    "    return opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'split':'test', 'dataset_name':'pcd', 'pkl_save_fname':'2020-01-13-argoverse_test_prediction.pkl', 'nsweeps':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-16 19:29:38,968 - INFO - Distributed testing: False\n",
      "2021-02-16 19:29:38,969 - INFO - torch.backends.cudnn.benchmark: False\n",
      "2021-02-16 19:29:39,022 - INFO - Finish RPN Initialization\n",
      "2021-02-16 19:29:39,022 - INFO - num_classes: [3]\n",
      "2021-02-16 19:29:39,031 - INFO - Finish CenterHead Initialization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model =  {'type': 'VoxelNet', 'pretrained': None, 'reader': {'type': 'VoxelFeatureExtractorV3', 'num_input_features': 5}, 'backbone': {'type': 'SpMiddleResNetFHD', 'num_input_features': 5, 'ds_factor': 8}, 'neck': {'type': 'RPN', 'layer_nums': [5, 5], 'ds_layer_strides': [1, 2], 'ds_num_filters': [128, 256], 'us_layer_strides': [1, 2], 'us_num_filters': [256, 256], 'num_input_features': 256, 'logger': <Logger RPN (INFO)>}, 'bbox_head': {'type': 'CenterHead', 'in_channels': 512, 'tasks': [{'num_class': 3, 'class_names': ['VEHICLE', 'PEDESTRIAN', 'CYCLIST']}], 'dataset': 'waymo', 'weight': 2, 'code_weights': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2, 1.0, 1.0], 'common_heads': {'reg': (2, 2), 'height': (1, 2), 'dim': (3, 2), 'rot': (2, 2), 'vel': (2, 2)}}}\n",
      "Use HM Bias:  -2.19\n",
      "Use Val Set\n",
      "Preprocess cfg =  namespace(mode='val', remove_environment=False, remove_unknown_examples=False, shuffle_points=False)\n",
      "\n",
      "                Voxel cfg: voxel_size - [0.1, 0.1, 0.15]; point_cloud_range - [-75.2, -75.2, -2, 75.2, 75.2, 4]; max_num_points - 5; max_voxels - [180000, 400000]\n",
      "            \n",
      "Using 2 sweeps\n",
      "Using 2 Frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-16 19:29:40,610 - INFO - work dir: work_dirs/mixed_wn\n"
     ]
    }
   ],
   "source": [
    "opts = load_opts()\n",
    "\n",
    "# pdb.set_trace()\n",
    "cfg = Config.fromfile(opts.config)\n",
    "cfg.local_rank = opts.local_rank\n",
    "\n",
    "# update configs according to CLI args\n",
    "if opts.work_dir is not None:\n",
    "    cfg.work_dir = opts.work_dir\n",
    "\n",
    "distributed = False\n",
    "if \"WORLD_SIZE\" in os.environ:\n",
    "    distributed = int(os.environ[\"WORLD_SIZE\"]) > 1\n",
    "\n",
    "cfg.gpus = opts.gpus\n",
    "\n",
    "# init logger before other steps\n",
    "logger = get_root_logger(cfg.log_level)\n",
    "logger.info(\"Distributed testing: {}\".format(distributed))\n",
    "logger.info(f\"torch.backends.cudnn.benchmark: {torch.backends.cudnn.benchmark}\")\n",
    "\n",
    "# pdb.set_trace()\n",
    "print('Model = ', cfg.model)\n",
    "model = build_detector(logger, cfg.model, train_cfg=None, test_cfg=cfg.test_cfg)\n",
    "\n",
    "if opts.testset:\n",
    "    print(\"Use Test Set\")\n",
    "    dataset = build_dataset(cfg.data.test, args)\n",
    "else:\n",
    "    print(\"Use Val Set\")\n",
    "    dataset = build_dataset(cfg.data.val, args)\n",
    "\n",
    "data_loader = build_dataloader(\n",
    "    dataset,\n",
    "    batch_size=cfg.data.samples_per_gpu if not opts.speed_test else 1,\n",
    "    workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "    dist=distributed,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# pdb.set_trace()\n",
    "checkpoint = load_checkpoint(model, opts.checkpoint, map_location=\"cpu\")\n",
    "\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "mode = \"val\"\n",
    "\n",
    "logger.info(f\"work dir: {opts.work_dir}\")\n",
    "# if cfg.local_rank == 0:\n",
    "#     prog_bar = torchie.ProgressBar(len(data_loader.dataset) // cfg.gpus)\n",
    "\n",
    "detections = {}\n",
    "cpu_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2\n",
      "1/2\n",
      "\n",
      " Total time per frame:  0.13686704635620117\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "start = int(len(dataset) / 3)\n",
    "end = int(len(dataset) * 2 /3)\n",
    "\n",
    "time_start = 0 \n",
    "time_end = 0 \n",
    "points_list = [] \n",
    "\n",
    "for i, data_batch in enumerate(data_loader):\n",
    "    \"\"\"\n",
    "    data_batch will have collated examples\n",
    "    so for double flip, get an extra dimension on the outside, indexed with 0, 1, 2, 3 etc for 4 examples\n",
    "    \"\"\"\n",
    "    print(f'{i}/{len(data_loader)}')\n",
    "    if i == start:\n",
    "        torch.cuda.synchronize()\n",
    "        time_start = time.time()\n",
    "\n",
    "    if i == end:\n",
    "        torch.cuda.synchronize()\n",
    "        time_end = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = batch_processor(\n",
    "            model, data_batch, train_mode=False, local_rank=opts.local_rank,\n",
    "        )\n",
    "    points = data_batch['points'][:, 1:4].cpu().numpy()\n",
    "    points_list.append(points.T)\n",
    "    for output in outputs:\n",
    "\n",
    "        # `output` is a dictionary with keys\n",
    "        # dict_keys(['box3d_lidar', 'scores', 'label_preds', 'metadata'])\n",
    "        # box3d_lidar is a tensor of shape [217, 9]\n",
    "        # scores is a tensor of shape [217]\n",
    "        # label_preds is a tensor of shape [217]\n",
    "        # metadata has {\n",
    "        #     'image_prefix': PosixPath('data/nuScenes/v1.0-test'),\n",
    "        #     'num_point_features': 5,\n",
    "        #     'token': '3c61eda50f694585bac0614e4660eca1'\n",
    "        #    }\n",
    "        token = output[\"metadata\"][\"token\"]\n",
    "        detections[token] = data_batch[\"annos\"]\n",
    "        for k, v in output.items():\n",
    "            if k not in [\"metadata\"]:\n",
    "                output[k] = v.to(cpu_device)\n",
    "        detections.update(\n",
    "            {token: output,}\n",
    "        )\n",
    "        detections[token][\"annos\"] = data_batch[\"annos\"]\n",
    "        # optionally dump the input!\n",
    "        # detections[token][\"input_coordinates\"] = data_batch[\"coordinates\"].to(cpu_device)\n",
    "        # detections[token][\"input_voxels\"] = data_batch[\"voxels\"].to(cpu_device)\n",
    "all_predictions = detections\n",
    "all_predictions = [all_predictions[i] for i in all_predictions]\n",
    "print(\"\\n Total time per frame: \", (time_end -  time_start) / (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEDESTRIAN,VEHICLE,VEHICLE,VEHICLE,VEHICLE,VEHICLE,PEDESTRIAN,VEHICLE,PEDESTRIAN,PEDESTRIAN,PEDESTRIAN,VEHICLE,VEHICLE,VEHICLE,VEHICLE,VEHICLE,VEHICLE,VEHICLE,PEDESTRIAN,VEHICLE,PEDESTRIAN,VEHICLE,PEDESTRIAN,VEHICLE,PEDESTRIAN,VEHICLE,PEDESTRIAN,VEHICLE,VEHICLE,VEHICLE,VEHICLE,VEHICLE,VEHICLE,PEDESTRIAN,VEHICLE,PEDESTRIAN,PEDESTRIAN,PEDESTRIAN,VEHICLE,VEHICLE,PEDESTRIAN,VEHICLE,PEDESTRIAN,CYCLIST,PEDESTRIAN,VEHICLE,PEDESTRIAN,PEDESTRIAN,"
     ]
    }
   ],
   "source": [
    "RED = (1,0,0)\n",
    "GREEN = (0,0.5,0)\n",
    "s = pd.Series(data=['VEHICLE', 'PEDESTRIAN', 'CYCLIST'], index=[0, 1, 2])\n",
    "for i in range(len(points_list)):\n",
    "    fig = draw_lidar(np.array(points_list[i]).T)\n",
    "    fig = draw_gt_boxes3d(convert_detbox_to_coordinates([all_predictions[i]])[0], fig, color=GREEN, labels= s[np.array(all_predictions[i]['label_preds'])].values, scores = all_predictions[i]['scores'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'box3d_lidar': tensor([[ 1.2645e+00, -1.1032e+01,  1.3959e+00,  7.6347e-01,  7.7269e-01,\n",
       "            1.7250e+00,  1.6645e-02, -8.2055e-01, -9.7078e-02],\n",
       "          [ 2.9259e+00,  1.0629e+01,  6.9083e-01,  1.9295e+00,  4.4831e+00,\n",
       "            1.4498e+00,  5.6300e-03, -1.5117e-04, -2.9115e-02],\n",
       "          [ 1.6247e+01,  6.9149e+00,  1.1888e+00,  2.5003e+00,  6.1090e+00,\n",
       "            2.7842e+00,  1.3063e-03, -1.8473e-04,  1.5698e+00],\n",
       "          [ 1.6648e+01,  3.3976e+01,  8.0695e-01,  1.9801e+00,  4.4940e+00,\n",
       "            1.6297e+00,  7.7214e-04, -2.9091e-04,  1.5930e+00],\n",
       "          [ 1.6166e+01,  3.1436e+01,  7.3657e-01,  2.0261e+00,  4.7112e+00,\n",
       "            1.5760e+00,  7.7214e-04, -2.9091e-04,  1.5873e+00],\n",
       "          [ 1.7128e+01,  3.6593e+01,  1.0299e+00,  2.2086e+00,  5.0573e+00,\n",
       "            2.0182e+00,  7.7214e-04, -2.9091e-04,  1.5980e+00],\n",
       "          [ 2.8574e+01,  2.7052e+01,  1.2595e+00,  7.0081e-01,  7.3627e-01,\n",
       "            1.7227e+00,  1.0292e-01,  1.2176e+00, -2.8538e+00],\n",
       "          [ 7.8207e+00, -1.0796e+01,  1.1404e+00,  1.8516e+00,  4.0309e+00,\n",
       "            1.6444e+00,  7.7214e-04, -2.9091e-04,  3.0063e-02],\n",
       "          [ 1.3010e+01,  3.5713e+01,  8.6578e-01,  7.6300e-01,  7.9756e-01,\n",
       "            1.7092e+00, -1.0432e-01,  1.1219e+00,  3.0539e+00],\n",
       "          [ 2.7879e+01,  2.7258e+01,  1.2769e+00,  6.6704e-01,  6.7446e-01,\n",
       "            1.6639e+00,  5.1159e-02,  9.4397e-01, -3.0321e+00],\n",
       "          [ 6.7313e-01,  4.1202e+01,  6.3042e-01,  7.2414e-01,  8.5134e-01,\n",
       "            1.7727e+00,  6.9649e-01, -1.3641e+00, -3.5252e-01],\n",
       "          [ 2.7085e+01,  4.2770e+01,  2.4985e-01,  1.8891e+00,  4.3554e+00,\n",
       "            1.5939e+00,  7.7214e-04, -2.9091e-04,  2.1254e+00],\n",
       "          [ 5.0157e+00, -1.0966e+01,  1.0801e+00,  1.8210e+00,  4.0285e+00,\n",
       "            1.6242e+00,  7.7214e-04, -2.9091e-04, -3.6613e-02],\n",
       "          [ 2.4311e+01,  3.4872e+01,  1.9494e+00,  2.7989e+00,  7.7208e+00,\n",
       "            3.7012e+00, -1.5976e+00, -4.0145e+00,  4.1447e-01],\n",
       "          [ 6.7389e+00,  3.6540e+01,  8.4300e-02,  1.8345e+00,  4.3771e+00,\n",
       "            1.5590e+00,  7.7214e-04, -2.9091e-04, -2.7968e+00],\n",
       "          [ 3.6899e+00,  3.3174e+01,  1.3011e+00,  2.8617e+00,  9.8210e+00,\n",
       "            3.4323e+00,  1.2933e-01, -4.3674e+00, -4.8371e-03],\n",
       "          [ 2.6700e+00,  6.4348e+01,  2.1832e+00,  2.7908e+00,  8.0029e+00,\n",
       "            3.4728e+00,  2.0071e-03, -1.1474e-03, -3.0803e+00],\n",
       "          [ 1.5696e+01,  4.1115e+01,  1.5760e-01,  1.8604e+00,  4.3140e+00,\n",
       "            1.5070e+00, -1.2654e+00,  7.6221e-01,  2.2492e+00],\n",
       "          [ 2.5767e+00,  3.6306e+01,  9.4977e-01,  6.7212e-01,  7.6007e-01,\n",
       "            1.7254e+00,  4.7454e-03,  1.5716e-01, -4.2178e-01],\n",
       "          [ 3.3188e+01,  7.0039e+00,  2.3965e-01,  2.6707e+00,  7.8864e+00,\n",
       "            3.0247e+00,  7.7214e-04, -2.9091e-04, -1.5553e+00],\n",
       "          [-2.5141e+00, -1.8282e+00, -4.6563e-02,  6.9236e-01,  1.3494e+00,\n",
       "            1.7312e+00, -7.9633e-03,  2.1256e-01, -2.9319e+00],\n",
       "          [ 2.6804e+01,  3.7875e+01,  1.2933e+00,  2.9512e+00,  1.3674e+01,\n",
       "            3.5562e+00, -4.3396e-01, -6.4627e-01,  1.1741e+00],\n",
       "          [ 7.2800e-02, -1.9666e+00,  6.4746e-01,  6.8497e-01,  7.3786e-01,\n",
       "            1.6247e+00,  1.2636e-02,  4.8513e-03, -1.7952e+00],\n",
       "          [ 8.4419e+00,  4.8507e+01,  6.4119e-01,  2.5834e+00,  8.0053e+00,\n",
       "            3.1668e+00, -2.9768e-04,  1.2663e-02,  3.0818e+00],\n",
       "          [ 2.7024e-01,  9.1787e+00,  1.0382e+00,  7.3545e-01,  7.6076e-01,\n",
       "            1.6351e+00,  5.3279e-03, -2.8849e-03, -3.6002e-01],\n",
       "          [ 4.9174e-01,  4.3492e+01,  5.0769e-01,  1.8863e+00,  4.4095e+00,\n",
       "            1.6574e+00,  1.5433e-01, -1.9072e-01, -2.0788e-01],\n",
       "          [ 2.8456e+00,  4.6199e+01,  2.2527e+00,  6.7648e-01,  9.1572e-01,\n",
       "            1.7575e+00,  2.2707e-01, -1.6974e+00, -9.3327e-02]]),\n",
       "  'scores': tensor([0.8070, 0.7966, 0.7798, 0.7616, 0.7443, 0.7241, 0.7195, 0.6334, 0.5876,\n",
       "          0.5685, 0.4859, 0.4666, 0.4575, 0.4003, 0.3902, 0.3859, 0.2794, 0.2633,\n",
       "          0.2494, 0.2398, 0.2323, 0.2305, 0.2270, 0.2237, 0.2236, 0.2229, 0.2228]),\n",
       "  'label_preds': tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "          1, 0, 1]),\n",
       "  'metadata': {'image_prefix': PosixPath('data/Panasonic/processed/lidar'),\n",
       "   'num_point_features': 5,\n",
       "   'token': 'data/Panasonic/processed/lidar/2021-01-26-12-28-49_Velodyne-HDL-64-Data_Frame_1098.pcd'},\n",
       "  'annos': array([None], dtype=object)},\n",
       " {'box3d_lidar': tensor([[ 1.6620e+01,  3.3928e+01,  8.0640e-01,  1.9715e+00,  4.5005e+00,\n",
       "            1.6223e+00,  7.7214e-04, -2.9091e-04,  1.6356e+00],\n",
       "          [ 2.9253e+00,  3.8611e+00,  7.1520e-01,  1.9285e+00,  4.4277e+00,\n",
       "            1.5195e+00,  1.0527e-01, -1.3489e+00, -5.5415e-02],\n",
       "          [ 1.6210e+01,  6.8966e+00,  1.1575e+00,  2.4331e+00,  5.8995e+00,\n",
       "            2.6383e+00,  7.7214e-04, -2.9091e-04,  1.5772e+00],\n",
       "          [ 1.7071e+01,  3.6502e+01,  1.1291e+00,  2.1565e+00,  4.8835e+00,\n",
       "            2.0396e+00,  7.7214e-04, -2.9091e-04,  1.6197e+00],\n",
       "          [ 1.6146e+01,  3.1438e+01,  7.3707e-01,  2.0186e+00,  4.7198e+00,\n",
       "            1.5516e+00,  7.7214e-04, -2.9091e-04,  1.5971e+00],\n",
       "          [ 2.3837e+01,  2.7608e+01,  1.5729e+00,  2.8686e+00,  9.5082e+00,\n",
       "            3.2217e+00, -1.8619e-02, -7.1066e-02, -3.2983e-03],\n",
       "          [ 7.4289e-01, -1.5576e+01,  1.6272e+00,  8.0544e-01,  8.4165e-01,\n",
       "            1.7604e+00,  3.7633e-01, -9.5646e-01, -2.9802e-01],\n",
       "          [ 7.7986e+00, -1.0881e+01,  1.1128e+00,  1.8621e+00,  4.0448e+00,\n",
       "            1.6613e+00,  7.7214e-04, -2.9091e-04,  2.1959e-02],\n",
       "          [ 1.0704e+01,  3.6589e+01,  7.4336e-01,  7.6343e-01,  1.1117e+00,\n",
       "            1.7657e+00, -1.2367e+00,  4.1688e-01,  1.7165e+00],\n",
       "          [ 2.8676e+01,  2.2333e+01,  1.5072e+00,  6.9306e-01,  7.0570e-01,\n",
       "            1.6730e+00,  1.3768e-01, -7.7622e-01, -2.2155e-01],\n",
       "          [ 6.7387e-01,  4.1200e+01,  6.7223e-01,  6.5579e-01,  7.0740e-01,\n",
       "            1.7245e+00,  1.3762e-01, -8.1133e-01, -3.2641e-01],\n",
       "          [ 4.9740e+00, -1.1002e+01,  1.0759e+00,  1.8112e+00,  4.0072e+00,\n",
       "            1.6346e+00,  7.7214e-04, -2.9091e-04, -2.7583e-02],\n",
       "          [ 3.5983e+00,  3.3140e+01,  1.2352e+00,  2.8064e+00,  9.4546e+00,\n",
       "            3.4508e+00,  3.6227e-01, -5.6803e+00, -3.3071e-02],\n",
       "          [-5.1515e-01, -1.6447e+01,  1.2956e+00,  7.1670e-01,  7.8904e-01,\n",
       "            1.6520e+00,  2.5749e-01, -7.3580e-02, -7.3265e-01],\n",
       "          [ 2.6390e+00,  6.3669e+01,  1.5695e+00,  2.3883e+00,  5.6569e+00,\n",
       "            2.7196e+00,  1.9454e-03, -1.9165e-03, -3.1257e+00],\n",
       "          [ 2.3363e-01,  9.1656e+00,  7.3362e-01,  6.9849e-01,  7.2240e-01,\n",
       "            1.7376e+00, -1.2451e-02, -7.0598e-01, -9.1019e-02],\n",
       "          [ 2.6514e+00,  3.6933e+01,  1.4046e+00,  7.8362e-01,  1.8675e+00,\n",
       "            1.9751e+00, -5.8522e-01,  1.6084e+00,  9.0871e-01],\n",
       "          [ 6.6765e-02, -1.9513e+00,  6.8147e-01,  6.5144e-01,  6.7006e-01,\n",
       "            1.6100e+00,  7.7214e-04, -2.9091e-04, -1.0139e+00],\n",
       "          [ 7.6334e+00,  4.5316e+01,  1.5843e+00,  2.7700e+00,  7.1634e+00,\n",
       "            3.5248e+00,  7.7214e-04, -2.9091e-04, -1.2553e+00],\n",
       "          [ 5.2388e+00, -1.6248e+01,  1.1710e+00,  6.7331e-01,  6.8510e-01,\n",
       "            1.6802e+00,  5.5199e-03,  6.4144e-04, -2.4099e+00],\n",
       "          [-2.5347e+00, -1.8396e+00,  1.5384e-02,  6.8852e-01,  8.7596e-01,\n",
       "            1.6699e+00,  8.7799e-04, -4.6564e-04, -1.9632e+00]]),\n",
       "  'scores': tensor([0.7866, 0.7720, 0.7659, 0.7346, 0.7303, 0.6157, 0.6064, 0.6019, 0.5257,\n",
       "          0.5043, 0.4735, 0.4269, 0.4170, 0.2800, 0.2694, 0.2677, 0.2617, 0.2580,\n",
       "          0.2506, 0.2402, 0.2295]),\n",
       "  'label_preds': tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 2, 1, 0, 1, 1]),\n",
       "  'metadata': {'image_prefix': PosixPath('data/Panasonic/processed/lidar'),\n",
       "   'num_point_features': 5,\n",
       "   'token': 'data/Panasonic/processed/lidar/2021-01-26-12-28-49_Velodyne-HDL-64-Data_Frame_1131.pcd'},\n",
       "  'annos': array([None], dtype=object)}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[all_predictions[i] for i in all_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lidar_simple(pc, color=None):\n",
    "    ''' Draw lidar points. simplest set up. '''\n",
    "    fig = mlab.figure(figure=None, bgcolor=(0,0,0), fgcolor=None, engine=None, size=(1600, 1000))\n",
    "    if color is None: color = pc[:,2]\n",
    "    #draw points\n",
    "    mlab.points3d(pc[:,0], pc[:,1], pc[:,2], color, color=None, mode='point', colormap = 'gnuplot', scale_factor=20, figure=fig)\n",
    "    #draw origin\n",
    "    mlab.points3d(0, 0, 0, color=(1,1,1), mode='sphere', scale_factor=0.2)\n",
    "    #draw axis\n",
    "    axes=np.array([\n",
    "        [2.,0.,0.,0.],\n",
    "        [0.,2.,0.,0.],\n",
    "        [0.,0.,2.,0.],\n",
    "    ],dtype=np.float64)\n",
    "    mlab.plot3d([0, axes[0,0]], [0, axes[0,1]], [0, axes[0,2]], color=(1,0,0), tube_radius=None, figure=fig)\n",
    "    mlab.plot3d([0, axes[1,0]], [0, axes[1,1]], [0, axes[1,2]], color=(0,1,0), tube_radius=None, figure=fig)\n",
    "    mlab.plot3d([0, axes[2,0]], [0, axes[2,1]], [0, axes[2,2]], color=(0,0,1), tube_radius=None, figure=fig)\n",
    "    mlab.view(azimuth=180, elevation=70, focalpoint=[ 12.0909996 , -1.04700089, -2.03249991], distance=62.0, figure=fig)\n",
    "    return fig\n",
    "\n",
    "def draw_lidar(pc, color=None, fig=None, bgcolor=(0,0,0), pts_scale=1, pts_mode='point', pts_color=None):\n",
    "    ''' Draw lidar points\n",
    "    Args:\n",
    "        pc: numpy array (n,3) of XYZ\n",
    "        color: numpy array (n) of intensity or whatever\n",
    "        fig: mayavi figure handler, if None create new one otherwise will use it\n",
    "    Returns:\n",
    "        fig: created or used fig\n",
    "    '''\n",
    "    if fig is None: fig = mlab.figure(figure=None, bgcolor=bgcolor, fgcolor=None, engine=None, size=(1600, 1000))\n",
    "    if color is None: color = pc[:,2]\n",
    "    mlab.points3d(pc[:,0], pc[:,1], pc[:,2], color, color=pts_color, mode=pts_mode, colormap = 'gnuplot', scale_factor=pts_scale, figure=fig)\n",
    "    \n",
    "    #draw origin\n",
    "    mlab.points3d(0, 0, 0, color=(1,1,1), mode='sphere', scale_factor=0.2)\n",
    "    \n",
    "    #draw axis\n",
    "    axes=np.array([\n",
    "        [2.,0.,0.,0.],\n",
    "        [0.,2.,0.,0.],\n",
    "        [0.,0.,2.,0.],\n",
    "    ],dtype=np.float64)\n",
    "    mlab.plot3d([0, axes[0,0]], [0, axes[0,1]], [0, axes[0,2]], color=(1,0,0), tube_radius=None, figure=fig)\n",
    "    mlab.plot3d([0, axes[1,0]], [0, axes[1,1]], [0, axes[1,2]], color=(0,1,0), tube_radius=None, figure=fig)\n",
    "    mlab.plot3d([0, axes[2,0]], [0, axes[2,1]], [0, axes[2,2]], color=(0,0,1), tube_radius=None, figure=fig)\n",
    "\n",
    "    # draw fov (todo: update to real sensor spec.)\n",
    "    fov=np.array([  # 45 degree\n",
    "        [20., 20., 0.,0.],\n",
    "        [20.,-20., 0.,0.],\n",
    "    ],dtype=np.float64)\n",
    "    \n",
    "    mlab.plot3d([0, fov[0,0]], [0, fov[0,1]], [0, fov[0,2]], color=(1,1,1), tube_radius=None, line_width=1, figure=fig)\n",
    "    mlab.plot3d([0, fov[1,0]], [0, fov[1,1]], [0, fov[1,2]], color=(1,1,1), tube_radius=None, line_width=1, figure=fig)\n",
    "   \n",
    "    # draw square region\n",
    "    TOP_Y_MIN=-20\n",
    "    TOP_Y_MAX=20\n",
    "    TOP_X_MIN=0\n",
    "    TOP_X_MAX=40\n",
    "    TOP_Z_MIN=-2.0\n",
    "    TOP_Z_MAX=0.4\n",
    "    \n",
    "    x1 = TOP_X_MIN\n",
    "    x2 = TOP_X_MAX\n",
    "    y1 = TOP_Y_MIN\n",
    "    y2 = TOP_Y_MAX\n",
    "    mlab.plot3d([x1, x1], [y1, y2], [0,0], color=(0.5,0.5,0.5), tube_radius=0.1, line_width=1, figure=fig)\n",
    "    mlab.plot3d([x2, x2], [y1, y2], [0,0], color=(0.5,0.5,0.5), tube_radius=0.1, line_width=1, figure=fig)\n",
    "    mlab.plot3d([x1, x2], [y1, y1], [0,0], color=(0.5,0.5,0.5), tube_radius=0.1, line_width=1, figure=fig)\n",
    "    mlab.plot3d([x1, x2], [y2, y2], [0,0], color=(0.5,0.5,0.5), tube_radius=0.1, line_width=1, figure=fig)\n",
    "    \n",
    "    #mlab.orientation_axes()\n",
    "    mlab.view(azimuth=180, elevation=70, focalpoint=[ 12.0909996 , -1.04700089, -2.03249991], distance=62.0, figure=fig)\n",
    "    return fig\n",
    "\n",
    "def draw_gt_boxes3d(gt_boxes3d, fig, color=(1,1,1), labels=None, scores=None, line_width=1, draw_text=True, text_scale=(0.4,0.4,0.4), color_list=None):\n",
    "    ''' Draw 3D bounding boxes\n",
    "    Args:\n",
    "        gt_boxes3d: numpy array (n,8,3) for XYZs of the box corners\n",
    "        fig: mayavi figure handler\n",
    "        color: RGB value tuple in range (0,1), box line color\n",
    "        line_width: box line width\n",
    "        draw_text: boolean, if true, write box indices beside boxes\n",
    "        text_scale: three number tuple\n",
    "        color_list: a list of RGB tuple, if not None, overwrite color.\n",
    "    Returns:\n",
    "        fig: updated fig\n",
    "    ''' \n",
    "    num = len(gt_boxes3d)\n",
    "    for n in range(num):\n",
    "        if(labels[n] in ['VEHICLE', 'CYCLIST', 'PEDESTRIAN']):\n",
    "            print(labels[n], end=',')\n",
    "            b = gt_boxes3d[n]\n",
    "            if color_list is not None:\n",
    "                color = color_list[n] \n",
    "            if scores is not None: \n",
    "                if(scores[n]<0.35):\n",
    "                    continue\n",
    "#                 print('printing scores too')\n",
    "                mlab.text3d(b[4,0], b[4,1], b[4,2], labels[n]+'{:.2f}'.format(scores[n]), scale=text_scale, color=color, figure=fig)\n",
    "            elif labels is not None: mlab.text3d(b[4,0], b[4,1], b[4,2], labels[n], scale=text_scale, color=color, figure=fig)\n",
    "            elif draw_text: mlab.text3d(b[4,0], b[4,1], b[4,2], '%d'%n, scale=text_scale, color=color, figure=fig)\n",
    "            for k in range(0,4):\n",
    "                #http://docs.enthought.com/mayavi/mayavi/auto/mlab_helper_functions.html\n",
    "                i,j=k,(k+1)%4\n",
    "                mlab.plot3d([b[i,0], b[j,0]], [b[i,1], b[j,1]], [b[i,2], b[j,2]], color=color, tube_radius=None, line_width=line_width, figure=fig)\n",
    "\n",
    "                i,j=k+4,(k+1)%4 + 4\n",
    "                mlab.plot3d([b[i,0], b[j,0]], [b[i,1], b[j,1]], [b[i,2], b[j,2]], color=color, tube_radius=None, line_width=line_width, figure=fig)\n",
    "\n",
    "                i,j=k,k+4\n",
    "                mlab.plot3d([b[i,0], b[j,0]], [b[i,1], b[j,1]], [b[i,2], b[j,2]], color=color, tube_radius=None, line_width=line_width, figure=fig)\n",
    "    #mlab.show(1)\n",
    "    #mlab.view(azimuth=180, elevation=70, focalpoint=[ 12.0909996 , -1.04700089, -2.03249991], distance=62.0, figure=fig)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_box_to_coordinates(gt_boxes3d):\n",
    "    new_gt_boxes3d = []\n",
    "    for det in gt_boxes3d:\n",
    "        intermediate = []\n",
    "        ndet = det['box3d_lidar']\n",
    "        try:\n",
    "            ndet = ndet.numpy()\n",
    "        except:\n",
    "            pass\n",
    "        for det in ndet:\n",
    "            box = []\n",
    "            center_x = det[0]\n",
    "            center_y = det[1]\n",
    "            center_z = det[2]\n",
    "            width = det[3]\n",
    "            length = det[4]\n",
    "            height = det[5]\n",
    "            det[-1] = -det[-1] - np.pi / 2\n",
    "            rotation_matrix = Quaternion(axis=[0, 0, 1], radians=det[-1]).rotation_matrix\n",
    "            ## 3D bounding box corners. (Convention: x points forward, y to the left, z up.)\n",
    "            x_corners = length / 2 * np.array([1,  1,  1,  1, -1, -1, -1, -1])\n",
    "            y_corners = width / 2 * np.array([1, -1, -1,  1,  1, -1, -1,  1])\n",
    "            z_corners = height / 2 * np.array([1,  1, -1, -1,  1,  1, -1, -1])\n",
    "            corners = np.vstack((x_corners, y_corners, z_corners))\n",
    "            ## Rotate\n",
    "            corners = np.dot(rotation_matrix, corners)\n",
    "            corners[0, :] = corners[0, :] + center_x\n",
    "            corners[1, :] = corners[1, :] + center_y\n",
    "            corners[2, :] = corners[2, :] + center_z\n",
    "            intermediate.append(corners.T)\n",
    "        new_gt_boxes3d.append(np.array(intermediate))\n",
    "    return(np.array(new_gt_boxes3d))\n",
    "def convert_detbox_to_coordinates(gt_boxes3d):\n",
    "    new_gt_boxes3d = []\n",
    "    for det in gt_boxes3d:\n",
    "        intermediate = []\n",
    "        ndet = det['box3d_lidar']\n",
    "        try:\n",
    "            ndet = ndet.numpy()\n",
    "        except:\n",
    "            pass\n",
    "        for det in ndet:\n",
    "            box = []\n",
    "            center_x = det[0]\n",
    "            center_y = det[1]\n",
    "            center_z = det[2]\n",
    "            width = det[3]\n",
    "            length = det[4]\n",
    "            height = det[5]\n",
    "#             det[-1] = -det[-1] - np.pi / 2\n",
    "            rotation_matrix = Quaternion(axis=[0, 0, 1], radians=-det[-1] - np.pi / 2).rotation_matrix\n",
    "            x_corners = length / 2 * np.array([1,  1,  1,  1, -1, -1, -1, -1])\n",
    "            y_corners = width / 2 * np.array([1, -1, -1,  1,  1, -1, -1,  1])\n",
    "            z_corners = height / 2 * np.array([1,  1, -1, -1,  1,  1, -1, -1])\n",
    "            corners = np.vstack((x_corners, y_corners, z_corners))\n",
    "            ## Rotate\n",
    "            corners = np.dot(rotation_matrix, corners)\n",
    "            corners[0, :] = corners[0, :] + center_x\n",
    "            corners[1, :] = corners[1, :] + center_y\n",
    "            corners[2, :] = corners[2, :] + center_z\n",
    "            intermediate.append(corners.T)\n",
    "        new_gt_boxes3d.append(np.array(intermediate))\n",
    "    return(np.array(new_gt_boxes3d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('centerpoint': conda)",
   "language": "python",
   "name": "python361264bitcenterpointconda20733c3946d44e5b93ab1bbb744622b6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
